{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5305-Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Insurance Premium Prediction Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Annual Income</th>\n",
       "      <th>Marital Status</th>\n",
       "      <th>Number of Dependents</th>\n",
       "      <th>Education Level</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Health Score</th>\n",
       "      <th>Location</th>\n",
       "      <th>Policy Type</th>\n",
       "      <th>Previous Claims</th>\n",
       "      <th>Vehicle Age</th>\n",
       "      <th>Credit Score</th>\n",
       "      <th>Insurance Duration</th>\n",
       "      <th>Premium Amount</th>\n",
       "      <th>Policy Start Date</th>\n",
       "      <th>Customer Feedback</th>\n",
       "      <th>Smoking Status</th>\n",
       "      <th>Exercise Frequency</th>\n",
       "      <th>Property Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>99990.0</td>\n",
       "      <td>Married</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Master's</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.074627</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Comprehensive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>320.0</td>\n",
       "      <td>5</td>\n",
       "      <td>308.0</td>\n",
       "      <td>2022-12-10 15:21:39.078837</td>\n",
       "      <td>Poor</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Daily</td>\n",
       "      <td>Condo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>2867.0</td>\n",
       "      <td>Single</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.271335</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Comprehensive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>694.0</td>\n",
       "      <td>4</td>\n",
       "      <td>517.0</td>\n",
       "      <td>2023-01-31 15:21:39.078837</td>\n",
       "      <td>Good</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Monthly</td>\n",
       "      <td>House</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>30154.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.714909</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>Comprehensive</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16</td>\n",
       "      <td>652.0</td>\n",
       "      <td>8</td>\n",
       "      <td>849.0</td>\n",
       "      <td>2023-11-26 15:21:39.078837</td>\n",
       "      <td>Poor</td>\n",
       "      <td>No</td>\n",
       "      <td>Monthly</td>\n",
       "      <td>House</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>48371.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>0.0</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Self-Employed</td>\n",
       "      <td>25.346926</td>\n",
       "      <td>Rural</td>\n",
       "      <td>Comprehensive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11</td>\n",
       "      <td>330.0</td>\n",
       "      <td>7</td>\n",
       "      <td>927.0</td>\n",
       "      <td>2023-02-27 15:21:39.078837</td>\n",
       "      <td>Poor</td>\n",
       "      <td>No</td>\n",
       "      <td>Rarely</td>\n",
       "      <td>Condo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>54174.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>0.0</td>\n",
       "      <td>High School</td>\n",
       "      <td>Self-Employed</td>\n",
       "      <td>6.659499</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Comprehensive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>303.0</td>\n",
       "      <td>2020-11-25 15:21:39.078837</td>\n",
       "      <td>Poor</td>\n",
       "      <td>No</td>\n",
       "      <td>Rarely</td>\n",
       "      <td>Condo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age  Gender  Annual Income Marital Status  Number of Dependents  \\\n",
       "0  56.0    Male        99990.0        Married                   1.0   \n",
       "1  46.0    Male         2867.0         Single                   1.0   \n",
       "2  32.0  Female        30154.0       Divorced                   3.0   \n",
       "3  60.0  Female        48371.0       Divorced                   0.0   \n",
       "4  25.0  Female        54174.0       Divorced                   0.0   \n",
       "\n",
       "  Education Level     Occupation  Health Score  Location    Policy Type  \\\n",
       "0        Master's            NaN     31.074627     Urban  Comprehensive   \n",
       "1      Bachelor's            NaN     50.271335     Urban  Comprehensive   \n",
       "2      Bachelor's            NaN     14.714909  Suburban  Comprehensive   \n",
       "3             PhD  Self-Employed     25.346926     Rural  Comprehensive   \n",
       "4     High School  Self-Employed      6.659499     Urban  Comprehensive   \n",
       "\n",
       "   Previous Claims  Vehicle Age  Credit Score  Insurance Duration  \\\n",
       "0              NaN           13         320.0                   5   \n",
       "1              NaN            3         694.0                   4   \n",
       "2              2.0           16         652.0                   8   \n",
       "3              1.0           11         330.0                   7   \n",
       "4              NaN            9           NaN                   8   \n",
       "\n",
       "   Premium Amount           Policy Start Date Customer Feedback  \\\n",
       "0           308.0  2022-12-10 15:21:39.078837              Poor   \n",
       "1           517.0  2023-01-31 15:21:39.078837              Good   \n",
       "2           849.0  2023-11-26 15:21:39.078837              Poor   \n",
       "3           927.0  2023-02-27 15:21:39.078837              Poor   \n",
       "4           303.0  2020-11-25 15:21:39.078837              Poor   \n",
       "\n",
       "  Smoking Status Exercise Frequency Property Type  \n",
       "0            Yes              Daily         Condo  \n",
       "1            Yes            Monthly         House  \n",
       "2             No            Monthly         House  \n",
       "3             No             Rarely         Condo  \n",
       "4             No             Rarely         Condo  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age                      True\n",
      "Gender                  False\n",
      "Annual Income            True\n",
      "Marital Status           True\n",
      "Number of Dependents     True\n",
      "Education Level         False\n",
      "Occupation               True\n",
      "Health Score             True\n",
      "Location                False\n",
      "Policy Type             False\n",
      "Previous Claims          True\n",
      "Vehicle Age             False\n",
      "Credit Score             True\n",
      "Insurance Duration      False\n",
      "Premium Amount           True\n",
      "Policy Start Date       False\n",
      "Customer Feedback        True\n",
      "Smoking Status          False\n",
      "Exercise Frequency      False\n",
      "Property Type           False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 278860 entries, 0 to 278859\n",
      "Data columns (total 20 columns):\n",
      " #   Column                Non-Null Count   Dtype  \n",
      "---  ------                --------------   -----  \n",
      " 0   Age                   274175 non-null  float64\n",
      " 1   Gender                278860 non-null  object \n",
      " 2   Annual Income         264905 non-null  float64\n",
      " 3   Marital Status        273841 non-null  object \n",
      " 4   Number of Dependents  250974 non-null  float64\n",
      " 5   Education Level       278860 non-null  object \n",
      " 6   Occupation            197572 non-null  object \n",
      " 7   Health Score          268263 non-null  float64\n",
      " 8   Location              278860 non-null  object \n",
      " 9   Policy Type           278860 non-null  object \n",
      " 10  Previous Claims       197572 non-null  float64\n",
      " 11  Vehicle Age           278860 non-null  int64  \n",
      " 12  Credit Score          250974 non-null  float64\n",
      " 13  Insurance Duration    278860 non-null  int64  \n",
      " 14  Premium Amount        277019 non-null  float64\n",
      " 15  Policy Start Date     278860 non-null  object \n",
      " 16  Customer Feedback     260511 non-null  object \n",
      " 17  Smoking Status        278860 non-null  object \n",
      " 18  Exercise Frequency    278860 non-null  object \n",
      " 19  Property Type         278860 non-null  object \n",
      "dtypes: float64(7), int64(2), object(11)\n",
      "memory usage: 42.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Column  Missing Values  % Missing\n",
      "0                    Age            4685   1.680055\n",
      "1                 Gender               0   0.000000\n",
      "2          Annual Income           13955   5.004303\n",
      "3         Marital Status            5019   1.799828\n",
      "4   Number of Dependents           27886  10.000000\n",
      "5        Education Level               0   0.000000\n",
      "6             Occupation           81288  29.150111\n",
      "7           Health Score           10597   3.800115\n",
      "8               Location               0   0.000000\n",
      "9            Policy Type               0   0.000000\n",
      "10       Previous Claims           81288  29.150111\n",
      "11           Vehicle Age               0   0.000000\n",
      "12          Credit Score           27886  10.000000\n",
      "13    Insurance Duration               0   0.000000\n",
      "14        Premium Amount            1841   0.660188\n",
      "15     Policy Start Date               0   0.000000\n",
      "16     Customer Feedback           18349   6.580004\n",
      "17        Smoking Status               0   0.000000\n",
      "18    Exercise Frequency               0   0.000000\n",
      "19         Property Type               0   0.000000\n"
     ]
    }
   ],
   "source": [
    "# Summarize the total missing values and their percentage\n",
    "missing_summary = df.isnull().sum().reset_index()\n",
    "missing_summary.columns = ['Column', 'Missing Values']\n",
    "missing_summary['% Missing'] = (missing_summary['Missing Values'] / len(df)) * 100\n",
    "print(missing_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a custom `InsuranceDataset` class that inherits the `Dataset` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InsuranceDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = pd.read_csv('./insurance_data_imputed.csv')\n",
    "        self.X = self.data.drop('premium_amount', axis=1)\n",
    "        self.y = self.data['premium_amount']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X.iloc[idx].values), torch.tensor(self.y.iloc[idx])\n",
    "\n",
    "dataset = InsuranceDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'gender', 'annual_income', 'marital_status',\n",
       "       'number_of_dependents', 'education_level', 'health_score', 'location',\n",
       "       'policy_type', 'previous_claims', 'credit_score', 'insurance_duration',\n",
       "       'smoking_status', 'exercise_frequency', 'occupation_employed',\n",
       "       'occupation_self_employed', 'occupation_unemployed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.X.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feedforward Neural Network with 2 hidden layers, with 64 and 32 neurons respectively\n",
    "\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(17, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "\n",
    "        self.output = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "\n",
    "        return self.output(x)\n",
    "\n",
    "model = FeedForwardNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training, testing and validation data\n",
    "train_size = int(0.7 * len(dataset))\n",
    "test_size = int(0.15 * len(dataset))\n",
    "val_size = len(dataset) - train_size - test_size\n",
    "\n",
    "train_dataset, test_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, test_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [100/4091], Loss: 1010956.0000\n",
      "Epoch [1/100], Step [200/4091], Loss: 483516.0312\n",
      "Epoch [1/100], Step [300/4091], Loss: 597812.9375\n",
      "Epoch [1/100], Step [400/4091], Loss: 1196385.0000\n",
      "Epoch [1/100], Step [500/4091], Loss: 1412062.6250\n",
      "Epoch [1/100], Step [600/4091], Loss: 1113873.6250\n",
      "Epoch [1/100], Step [700/4091], Loss: 353983.6875\n",
      "Epoch [1/100], Step [800/4091], Loss: 883291.5000\n",
      "Epoch [1/100], Step [900/4091], Loss: 590406.2500\n",
      "Epoch [1/100], Step [1000/4091], Loss: 1072617.2500\n",
      "Epoch [1/100], Step [1100/4091], Loss: 453500.6875\n",
      "Epoch [1/100], Step [1200/4091], Loss: 1051424.0000\n",
      "Epoch [1/100], Step [1300/4091], Loss: 1311473.3750\n",
      "Epoch [1/100], Step [1400/4091], Loss: 808052.1875\n",
      "Epoch [1/100], Step [1500/4091], Loss: 466269.2188\n",
      "Epoch [1/100], Step [1600/4091], Loss: 731063.5000\n",
      "Epoch [1/100], Step [1700/4091], Loss: 1492303.1250\n",
      "Epoch [1/100], Step [1800/4091], Loss: 1060279.3750\n",
      "Epoch [1/100], Step [1900/4091], Loss: 493483.1250\n",
      "Epoch [1/100], Step [2000/4091], Loss: 702704.0000\n",
      "Epoch [1/100], Step [2100/4091], Loss: 684059.3750\n",
      "Epoch [1/100], Step [2200/4091], Loss: 904171.5000\n",
      "Epoch [1/100], Step [2300/4091], Loss: 725781.3125\n",
      "Epoch [1/100], Step [2400/4091], Loss: 485206.3750\n",
      "Epoch [1/100], Step [2500/4091], Loss: 1512039.0000\n",
      "Epoch [1/100], Step [2600/4091], Loss: 839126.3125\n",
      "Epoch [1/100], Step [2700/4091], Loss: 840709.0625\n",
      "Epoch [1/100], Step [2800/4091], Loss: 727803.5000\n",
      "Epoch [1/100], Step [2900/4091], Loss: 1331246.3750\n",
      "Epoch [1/100], Step [3000/4091], Loss: 532831.9375\n",
      "Epoch [1/100], Step [3100/4091], Loss: 1726311.6250\n",
      "Epoch [1/100], Step [3200/4091], Loss: 790721.7500\n",
      "Epoch [1/100], Step [3300/4091], Loss: 476662.5000\n",
      "Epoch [1/100], Step [3400/4091], Loss: 946963.3750\n",
      "Epoch [1/100], Step [3500/4091], Loss: 534010.1875\n",
      "Epoch [1/100], Step [3600/4091], Loss: 694473.0000\n",
      "Epoch [1/100], Step [3700/4091], Loss: 1259917.3750\n",
      "Epoch [1/100], Step [3800/4091], Loss: 685598.6875\n",
      "Epoch [1/100], Step [3900/4091], Loss: 1248537.2500\n",
      "Epoch [1/100], Step [4000/4091], Loss: 612958.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([14])) that is different to the input size (torch.Size([14, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Step [100/4091], Loss: 1355551.0000\n",
      "Epoch [2/100], Step [200/4091], Loss: 869529.8125\n",
      "Epoch [2/100], Step [300/4091], Loss: 473942.7812\n",
      "Epoch [2/100], Step [400/4091], Loss: 790107.2500\n",
      "Epoch [2/100], Step [500/4091], Loss: 431988.9688\n",
      "Epoch [2/100], Step [600/4091], Loss: 578378.8750\n",
      "Epoch [2/100], Step [700/4091], Loss: 864970.5000\n",
      "Epoch [2/100], Step [800/4091], Loss: 1219696.2500\n",
      "Epoch [2/100], Step [900/4091], Loss: 596032.0000\n",
      "Epoch [2/100], Step [1000/4091], Loss: 756629.3750\n",
      "Epoch [2/100], Step [1100/4091], Loss: 466924.3125\n",
      "Epoch [2/100], Step [1200/4091], Loss: 859175.6875\n",
      "Epoch [2/100], Step [1300/4091], Loss: 1272061.5000\n",
      "Epoch [2/100], Step [1400/4091], Loss: 938838.1250\n",
      "Epoch [2/100], Step [1500/4091], Loss: 1429394.7500\n",
      "Epoch [2/100], Step [1600/4091], Loss: 698217.8750\n",
      "Epoch [2/100], Step [1700/4091], Loss: 1119081.1250\n",
      "Epoch [2/100], Step [1800/4091], Loss: 597102.1250\n",
      "Epoch [2/100], Step [1900/4091], Loss: 474654.5000\n",
      "Epoch [2/100], Step [2000/4091], Loss: 934985.6250\n",
      "Epoch [2/100], Step [2100/4091], Loss: 798109.5000\n",
      "Epoch [2/100], Step [2200/4091], Loss: 654773.5000\n",
      "Epoch [2/100], Step [2300/4091], Loss: 778951.1250\n",
      "Epoch [2/100], Step [2400/4091], Loss: 591754.8750\n",
      "Epoch [2/100], Step [2500/4091], Loss: 828650.3750\n",
      "Epoch [2/100], Step [2600/4091], Loss: 668922.7500\n",
      "Epoch [2/100], Step [2700/4091], Loss: 707424.0000\n",
      "Epoch [2/100], Step [2800/4091], Loss: 608270.0625\n",
      "Epoch [2/100], Step [2900/4091], Loss: 453569.0625\n",
      "Epoch [2/100], Step [3000/4091], Loss: 776038.5625\n",
      "Epoch [2/100], Step [3100/4091], Loss: 494081.2500\n",
      "Epoch [2/100], Step [3200/4091], Loss: 1039822.3750\n",
      "Epoch [2/100], Step [3300/4091], Loss: 530203.3750\n",
      "Epoch [2/100], Step [3400/4091], Loss: 1056165.2500\n",
      "Epoch [2/100], Step [3500/4091], Loss: 1019376.6250\n",
      "Epoch [2/100], Step [3600/4091], Loss: 875129.5625\n",
      "Epoch [2/100], Step [3700/4091], Loss: 937830.7500\n",
      "Epoch [2/100], Step [3800/4091], Loss: 731113.3125\n",
      "Epoch [2/100], Step [3900/4091], Loss: 369171.4375\n",
      "Epoch [2/100], Step [4000/4091], Loss: 798554.8125\n",
      "Epoch [3/100], Step [100/4091], Loss: 740888.6875\n",
      "Epoch [3/100], Step [200/4091], Loss: 834447.2500\n",
      "Epoch [3/100], Step [300/4091], Loss: 690511.6250\n",
      "Epoch [3/100], Step [400/4091], Loss: 899145.8750\n",
      "Epoch [3/100], Step [500/4091], Loss: 894337.9375\n",
      "Epoch [3/100], Step [600/4091], Loss: 851160.0000\n",
      "Epoch [3/100], Step [700/4091], Loss: 606318.0000\n",
      "Epoch [3/100], Step [800/4091], Loss: 757691.0000\n",
      "Epoch [3/100], Step [900/4091], Loss: 1109462.8750\n",
      "Epoch [3/100], Step [1000/4091], Loss: 558496.0000\n",
      "Epoch [3/100], Step [1100/4091], Loss: 1144298.0000\n",
      "Epoch [3/100], Step [1200/4091], Loss: 1580704.7500\n",
      "Epoch [3/100], Step [1300/4091], Loss: 1036255.3750\n",
      "Epoch [3/100], Step [1400/4091], Loss: 507337.5938\n",
      "Epoch [3/100], Step [1500/4091], Loss: 373656.8750\n",
      "Epoch [3/100], Step [1600/4091], Loss: 1041535.4375\n",
      "Epoch [3/100], Step [1700/4091], Loss: 583241.4375\n",
      "Epoch [3/100], Step [1800/4091], Loss: 1041095.8750\n",
      "Epoch [3/100], Step [1900/4091], Loss: 938134.8750\n",
      "Epoch [3/100], Step [2000/4091], Loss: 1019019.3125\n",
      "Epoch [3/100], Step [2100/4091], Loss: 591746.2500\n",
      "Epoch [3/100], Step [2200/4091], Loss: 901409.5000\n",
      "Epoch [3/100], Step [2300/4091], Loss: 1192907.8750\n",
      "Epoch [3/100], Step [2400/4091], Loss: 1132309.1250\n",
      "Epoch [3/100], Step [2500/4091], Loss: 417003.2812\n",
      "Epoch [3/100], Step [2600/4091], Loss: 1015097.8750\n",
      "Epoch [3/100], Step [2700/4091], Loss: 501245.5000\n",
      "Epoch [3/100], Step [2800/4091], Loss: 898571.5000\n",
      "Epoch [3/100], Step [2900/4091], Loss: 474540.5312\n",
      "Epoch [3/100], Step [3000/4091], Loss: 828679.6250\n",
      "Epoch [3/100], Step [3100/4091], Loss: 925610.5000\n",
      "Epoch [3/100], Step [3200/4091], Loss: 1263912.0000\n",
      "Epoch [3/100], Step [3300/4091], Loss: 942708.1875\n",
      "Epoch [3/100], Step [3400/4091], Loss: 667692.6875\n",
      "Epoch [3/100], Step [3500/4091], Loss: 372004.2188\n",
      "Epoch [3/100], Step [3600/4091], Loss: 1085648.3750\n",
      "Epoch [3/100], Step [3700/4091], Loss: 621664.6875\n",
      "Epoch [3/100], Step [3800/4091], Loss: 761751.5000\n",
      "Epoch [3/100], Step [3900/4091], Loss: 400825.2812\n",
      "Epoch [3/100], Step [4000/4091], Loss: 1177047.2500\n",
      "Epoch [4/100], Step [100/4091], Loss: 947789.0625\n",
      "Epoch [4/100], Step [200/4091], Loss: 795302.0000\n",
      "Epoch [4/100], Step [300/4091], Loss: 935475.1250\n",
      "Epoch [4/100], Step [400/4091], Loss: 623222.7500\n",
      "Epoch [4/100], Step [500/4091], Loss: 980774.0000\n",
      "Epoch [4/100], Step [600/4091], Loss: 1341769.0000\n",
      "Epoch [4/100], Step [700/4091], Loss: 1006709.2500\n",
      "Epoch [4/100], Step [800/4091], Loss: 630098.5000\n",
      "Epoch [4/100], Step [900/4091], Loss: 644925.6250\n",
      "Epoch [4/100], Step [1000/4091], Loss: 1274561.8750\n",
      "Epoch [4/100], Step [1100/4091], Loss: 658086.1250\n",
      "Epoch [4/100], Step [1200/4091], Loss: 1113345.0000\n",
      "Epoch [4/100], Step [1300/4091], Loss: 771698.1875\n",
      "Epoch [4/100], Step [1400/4091], Loss: 338570.6875\n",
      "Epoch [4/100], Step [1500/4091], Loss: 511415.1875\n",
      "Epoch [4/100], Step [1600/4091], Loss: 884900.2500\n",
      "Epoch [4/100], Step [1700/4091], Loss: 1007787.8750\n",
      "Epoch [4/100], Step [1800/4091], Loss: 1362638.6250\n",
      "Epoch [4/100], Step [1900/4091], Loss: 809428.2500\n",
      "Epoch [4/100], Step [2000/4091], Loss: 830158.4375\n",
      "Epoch [4/100], Step [2100/4091], Loss: 510452.6562\n",
      "Epoch [4/100], Step [2200/4091], Loss: 633261.5625\n",
      "Epoch [4/100], Step [2300/4091], Loss: 904125.0000\n",
      "Epoch [4/100], Step [2400/4091], Loss: 912694.3125\n",
      "Epoch [4/100], Step [2500/4091], Loss: 773059.8750\n",
      "Epoch [4/100], Step [2600/4091], Loss: 602372.0000\n",
      "Epoch [4/100], Step [2700/4091], Loss: 815240.1875\n",
      "Epoch [4/100], Step [2800/4091], Loss: 1679700.0000\n",
      "Epoch [4/100], Step [2900/4091], Loss: 796938.2500\n",
      "Epoch [4/100], Step [3000/4091], Loss: 768879.9375\n",
      "Epoch [4/100], Step [3100/4091], Loss: 814271.0000\n",
      "Epoch [4/100], Step [3200/4091], Loss: 1219515.5000\n",
      "Epoch [4/100], Step [3300/4091], Loss: 1115302.5000\n",
      "Epoch [4/100], Step [3400/4091], Loss: 627407.6875\n",
      "Epoch [4/100], Step [3500/4091], Loss: 732301.5000\n",
      "Epoch [4/100], Step [3600/4091], Loss: 716608.8750\n",
      "Epoch [4/100], Step [3700/4091], Loss: 914012.5000\n",
      "Epoch [4/100], Step [3800/4091], Loss: 1120706.5000\n",
      "Epoch [4/100], Step [3900/4091], Loss: 600153.0625\n",
      "Epoch [4/100], Step [4000/4091], Loss: 482996.2500\n",
      "Epoch [5/100], Step [100/4091], Loss: 468111.2812\n",
      "Epoch [5/100], Step [200/4091], Loss: 435970.3750\n",
      "Epoch [5/100], Step [300/4091], Loss: 959156.6250\n",
      "Epoch [5/100], Step [400/4091], Loss: 934918.0625\n",
      "Epoch [5/100], Step [500/4091], Loss: 410349.8750\n",
      "Epoch [5/100], Step [600/4091], Loss: 613496.3750\n",
      "Epoch [5/100], Step [700/4091], Loss: 813318.5000\n",
      "Epoch [5/100], Step [800/4091], Loss: 392165.1250\n",
      "Epoch [5/100], Step [900/4091], Loss: 1375870.5000\n",
      "Epoch [5/100], Step [1000/4091], Loss: 1027236.5000\n",
      "Epoch [5/100], Step [1100/4091], Loss: 1030439.5625\n",
      "Epoch [5/100], Step [1200/4091], Loss: 791827.9375\n",
      "Epoch [5/100], Step [1300/4091], Loss: 660898.9375\n",
      "Epoch [5/100], Step [1400/4091], Loss: 581497.2500\n",
      "Epoch [5/100], Step [1500/4091], Loss: 431109.3750\n",
      "Epoch [5/100], Step [1600/4091], Loss: 522907.6562\n",
      "Epoch [5/100], Step [1700/4091], Loss: 297762.8750\n",
      "Epoch [5/100], Step [1800/4091], Loss: 403712.2812\n",
      "Epoch [5/100], Step [1900/4091], Loss: 779662.5625\n",
      "Epoch [5/100], Step [2000/4091], Loss: 804167.7500\n",
      "Epoch [5/100], Step [2100/4091], Loss: 420323.7500\n",
      "Epoch [5/100], Step [2200/4091], Loss: 631828.8125\n",
      "Epoch [5/100], Step [2300/4091], Loss: 616030.1875\n",
      "Epoch [5/100], Step [2400/4091], Loss: 964920.9375\n",
      "Epoch [5/100], Step [2500/4091], Loss: 912215.2500\n",
      "Epoch [5/100], Step [2600/4091], Loss: 711785.1250\n",
      "Epoch [5/100], Step [2700/4091], Loss: 1144567.1250\n",
      "Epoch [5/100], Step [2800/4091], Loss: 781243.5625\n",
      "Epoch [5/100], Step [2900/4091], Loss: 324004.5000\n",
      "Epoch [5/100], Step [3000/4091], Loss: 1078904.0000\n",
      "Epoch [5/100], Step [3100/4091], Loss: 716171.6250\n",
      "Epoch [5/100], Step [3200/4091], Loss: 1403250.2500\n",
      "Epoch [5/100], Step [3300/4091], Loss: 1435386.6250\n",
      "Epoch [5/100], Step [3400/4091], Loss: 1163907.6250\n",
      "Epoch [5/100], Step [3500/4091], Loss: 786658.6875\n",
      "Epoch [5/100], Step [3600/4091], Loss: 954853.8750\n",
      "Epoch [5/100], Step [3700/4091], Loss: 486073.0000\n",
      "Epoch [5/100], Step [3800/4091], Loss: 1404633.5000\n",
      "Epoch [5/100], Step [3900/4091], Loss: 789537.9375\n",
      "Epoch [5/100], Step [4000/4091], Loss: 816087.5000\n",
      "Epoch [6/100], Step [100/4091], Loss: 687277.3125\n",
      "Epoch [6/100], Step [200/4091], Loss: 401070.6875\n",
      "Epoch [6/100], Step [300/4091], Loss: 1013619.9375\n",
      "Epoch [6/100], Step [400/4091], Loss: 1037800.5000\n",
      "Epoch [6/100], Step [500/4091], Loss: 1114430.7500\n",
      "Epoch [6/100], Step [600/4091], Loss: 724148.1875\n",
      "Epoch [6/100], Step [700/4091], Loss: 780405.0000\n",
      "Epoch [6/100], Step [800/4091], Loss: 742858.9375\n",
      "Epoch [6/100], Step [900/4091], Loss: 773256.8750\n",
      "Epoch [6/100], Step [1000/4091], Loss: 514534.4375\n",
      "Epoch [6/100], Step [1100/4091], Loss: 540077.6875\n",
      "Epoch [6/100], Step [1200/4091], Loss: 974137.4375\n",
      "Epoch [6/100], Step [1300/4091], Loss: 488148.3438\n",
      "Epoch [6/100], Step [1400/4091], Loss: 403022.8438\n",
      "Epoch [6/100], Step [1500/4091], Loss: 638612.1250\n",
      "Epoch [6/100], Step [1600/4091], Loss: 569171.1250\n",
      "Epoch [6/100], Step [1700/4091], Loss: 1140960.5000\n",
      "Epoch [6/100], Step [1800/4091], Loss: 1157279.0000\n",
      "Epoch [6/100], Step [1900/4091], Loss: 716493.0625\n",
      "Epoch [6/100], Step [2000/4091], Loss: 820897.8750\n",
      "Epoch [6/100], Step [2100/4091], Loss: 750220.8750\n",
      "Epoch [6/100], Step [2200/4091], Loss: 1016972.5625\n",
      "Epoch [6/100], Step [2300/4091], Loss: 635133.0000\n",
      "Epoch [6/100], Step [2400/4091], Loss: 870545.8750\n",
      "Epoch [6/100], Step [2500/4091], Loss: 999974.9375\n",
      "Epoch [6/100], Step [2600/4091], Loss: 852671.9375\n",
      "Epoch [6/100], Step [2700/4091], Loss: 932905.1250\n",
      "Epoch [6/100], Step [2800/4091], Loss: 964835.3125\n",
      "Epoch [6/100], Step [2900/4091], Loss: 1134644.3750\n",
      "Epoch [6/100], Step [3000/4091], Loss: 508969.5312\n",
      "Epoch [6/100], Step [3100/4091], Loss: 852901.6875\n",
      "Epoch [6/100], Step [3200/4091], Loss: 491880.6875\n",
      "Epoch [6/100], Step [3300/4091], Loss: 555447.0000\n",
      "Epoch [6/100], Step [3400/4091], Loss: 723411.3750\n",
      "Epoch [6/100], Step [3500/4091], Loss: 1400433.7500\n",
      "Epoch [6/100], Step [3600/4091], Loss: 1178876.1250\n",
      "Epoch [6/100], Step [3700/4091], Loss: 960545.7500\n",
      "Epoch [6/100], Step [3800/4091], Loss: 773418.1875\n",
      "Epoch [6/100], Step [3900/4091], Loss: 844196.8125\n",
      "Epoch [6/100], Step [4000/4091], Loss: 917864.3750\n",
      "Epoch [7/100], Step [100/4091], Loss: 664975.1875\n",
      "Epoch [7/100], Step [200/4091], Loss: 505017.5938\n",
      "Epoch [7/100], Step [300/4091], Loss: 1177415.5000\n",
      "Epoch [7/100], Step [400/4091], Loss: 591528.0625\n",
      "Epoch [7/100], Step [500/4091], Loss: 598536.3750\n",
      "Epoch [7/100], Step [600/4091], Loss: 1118819.5000\n",
      "Epoch [7/100], Step [700/4091], Loss: 879209.0625\n",
      "Epoch [7/100], Step [800/4091], Loss: 608431.4375\n",
      "Epoch [7/100], Step [900/4091], Loss: 965135.0000\n",
      "Epoch [7/100], Step [1000/4091], Loss: 552482.7500\n",
      "Epoch [7/100], Step [1100/4091], Loss: 539420.5000\n",
      "Epoch [7/100], Step [1200/4091], Loss: 583633.8125\n",
      "Epoch [7/100], Step [1300/4091], Loss: 729242.6250\n",
      "Epoch [7/100], Step [1400/4091], Loss: 1016734.6250\n",
      "Epoch [7/100], Step [1500/4091], Loss: 914446.1250\n",
      "Epoch [7/100], Step [1600/4091], Loss: 888519.0000\n",
      "Epoch [7/100], Step [1700/4091], Loss: 630691.5625\n",
      "Epoch [7/100], Step [1800/4091], Loss: 1010903.1875\n",
      "Epoch [7/100], Step [1900/4091], Loss: 461086.6562\n",
      "Epoch [7/100], Step [2000/4091], Loss: 1339482.3750\n",
      "Epoch [7/100], Step [2100/4091], Loss: 606920.7500\n",
      "Epoch [7/100], Step [2200/4091], Loss: 691308.1250\n",
      "Epoch [7/100], Step [2300/4091], Loss: 581379.3750\n",
      "Epoch [7/100], Step [2400/4091], Loss: 1824398.3750\n",
      "Epoch [7/100], Step [2500/4091], Loss: 699652.0625\n",
      "Epoch [7/100], Step [2600/4091], Loss: 697739.6250\n",
      "Epoch [7/100], Step [2700/4091], Loss: 955249.7500\n",
      "Epoch [7/100], Step [2800/4091], Loss: 544497.5625\n",
      "Epoch [7/100], Step [2900/4091], Loss: 588814.6250\n",
      "Epoch [7/100], Step [3000/4091], Loss: 672493.1250\n",
      "Epoch [7/100], Step [3100/4091], Loss: 763709.5000\n",
      "Epoch [7/100], Step [3200/4091], Loss: 503937.3125\n",
      "Epoch [7/100], Step [3300/4091], Loss: 390089.2500\n",
      "Epoch [7/100], Step [3400/4091], Loss: 740956.1250\n",
      "Epoch [7/100], Step [3500/4091], Loss: 1100308.8750\n",
      "Epoch [7/100], Step [3600/4091], Loss: 458468.0312\n",
      "Epoch [7/100], Step [3700/4091], Loss: 1306560.7500\n",
      "Epoch [7/100], Step [3800/4091], Loss: 1233872.8750\n",
      "Epoch [7/100], Step [3900/4091], Loss: 844851.2500\n",
      "Epoch [7/100], Step [4000/4091], Loss: 516596.2812\n",
      "Epoch [8/100], Step [100/4091], Loss: 800737.3750\n",
      "Epoch [8/100], Step [200/4091], Loss: 1727627.7500\n",
      "Epoch [8/100], Step [300/4091], Loss: 647856.1875\n",
      "Epoch [8/100], Step [400/4091], Loss: 1469482.1250\n",
      "Epoch [8/100], Step [500/4091], Loss: 513250.8750\n",
      "Epoch [8/100], Step [600/4091], Loss: 471544.7812\n",
      "Epoch [8/100], Step [700/4091], Loss: 411735.9375\n",
      "Epoch [8/100], Step [800/4091], Loss: 533978.2500\n",
      "Epoch [8/100], Step [900/4091], Loss: 496852.2500\n",
      "Epoch [8/100], Step [1000/4091], Loss: 636747.6250\n",
      "Epoch [8/100], Step [1100/4091], Loss: 592948.0000\n",
      "Epoch [8/100], Step [1200/4091], Loss: 1611657.6250\n",
      "Epoch [8/100], Step [1300/4091], Loss: 520453.1875\n",
      "Epoch [8/100], Step [1400/4091], Loss: 852232.6875\n",
      "Epoch [8/100], Step [1500/4091], Loss: 603342.7500\n",
      "Epoch [8/100], Step [1600/4091], Loss: 987458.9375\n",
      "Epoch [8/100], Step [1700/4091], Loss: 1165246.7500\n",
      "Epoch [8/100], Step [1800/4091], Loss: 842659.7500\n",
      "Epoch [8/100], Step [1900/4091], Loss: 644681.1250\n",
      "Epoch [8/100], Step [2000/4091], Loss: 1157390.8750\n",
      "Epoch [8/100], Step [2100/4091], Loss: 1051863.6250\n",
      "Epoch [8/100], Step [2200/4091], Loss: 933205.5625\n",
      "Epoch [8/100], Step [2300/4091], Loss: 829717.8125\n",
      "Epoch [8/100], Step [2400/4091], Loss: 495154.8125\n",
      "Epoch [8/100], Step [2500/4091], Loss: 1020708.6875\n",
      "Epoch [8/100], Step [2600/4091], Loss: 1022709.0000\n",
      "Epoch [8/100], Step [2700/4091], Loss: 477677.4062\n",
      "Epoch [8/100], Step [2800/4091], Loss: 657437.0000\n",
      "Epoch [8/100], Step [2900/4091], Loss: 607600.9375\n",
      "Epoch [8/100], Step [3000/4091], Loss: 655229.5000\n",
      "Epoch [8/100], Step [3100/4091], Loss: 1472148.6250\n",
      "Epoch [8/100], Step [3200/4091], Loss: 764125.8750\n",
      "Epoch [8/100], Step [3300/4091], Loss: 343522.4375\n",
      "Epoch [8/100], Step [3400/4091], Loss: 901827.8125\n",
      "Epoch [8/100], Step [3500/4091], Loss: 1025212.8750\n",
      "Epoch [8/100], Step [3600/4091], Loss: 985177.4375\n",
      "Epoch [8/100], Step [3700/4091], Loss: 971286.3125\n",
      "Epoch [8/100], Step [3800/4091], Loss: 573029.3125\n",
      "Epoch [8/100], Step [3900/4091], Loss: 673734.0000\n",
      "Epoch [8/100], Step [4000/4091], Loss: 315101.1875\n",
      "Epoch [9/100], Step [100/4091], Loss: 1454688.3750\n",
      "Epoch [9/100], Step [200/4091], Loss: 740276.1250\n",
      "Epoch [9/100], Step [300/4091], Loss: 687695.6875\n",
      "Epoch [9/100], Step [400/4091], Loss: 683626.1250\n",
      "Epoch [9/100], Step [500/4091], Loss: 397721.4375\n",
      "Epoch [9/100], Step [600/4091], Loss: 617255.0000\n",
      "Epoch [9/100], Step [700/4091], Loss: 1275103.7500\n",
      "Epoch [9/100], Step [800/4091], Loss: 855782.8750\n",
      "Epoch [9/100], Step [900/4091], Loss: 670043.5625\n",
      "Epoch [9/100], Step [1000/4091], Loss: 861682.7500\n",
      "Epoch [9/100], Step [1100/4091], Loss: 663562.4375\n",
      "Epoch [9/100], Step [1200/4091], Loss: 1077790.5000\n",
      "Epoch [9/100], Step [1300/4091], Loss: 1523065.3750\n",
      "Epoch [9/100], Step [1400/4091], Loss: 352455.4375\n",
      "Epoch [9/100], Step [1500/4091], Loss: 724885.2500\n",
      "Epoch [9/100], Step [1600/4091], Loss: 1498272.7500\n",
      "Epoch [9/100], Step [1700/4091], Loss: 799049.8750\n",
      "Epoch [9/100], Step [1800/4091], Loss: 1010600.3750\n",
      "Epoch [9/100], Step [1900/4091], Loss: 1176927.2500\n",
      "Epoch [9/100], Step [2000/4091], Loss: 1206876.5000\n",
      "Epoch [9/100], Step [2100/4091], Loss: 695870.3750\n",
      "Epoch [9/100], Step [2200/4091], Loss: 1051595.2500\n",
      "Epoch [9/100], Step [2300/4091], Loss: 1240110.7500\n",
      "Epoch [9/100], Step [2400/4091], Loss: 1120627.0000\n",
      "Epoch [9/100], Step [2500/4091], Loss: 995259.0625\n",
      "Epoch [9/100], Step [2600/4091], Loss: 587365.0625\n",
      "Epoch [9/100], Step [2700/4091], Loss: 672469.4375\n",
      "Epoch [9/100], Step [2800/4091], Loss: 1091087.7500\n",
      "Epoch [9/100], Step [2900/4091], Loss: 1184425.6250\n",
      "Epoch [9/100], Step [3000/4091], Loss: 736419.1875\n",
      "Epoch [9/100], Step [3100/4091], Loss: 471037.2188\n",
      "Epoch [9/100], Step [3200/4091], Loss: 871600.0000\n",
      "Epoch [9/100], Step [3300/4091], Loss: 663200.3750\n",
      "Epoch [9/100], Step [3400/4091], Loss: 713628.5000\n",
      "Epoch [9/100], Step [3500/4091], Loss: 1346482.8750\n",
      "Epoch [9/100], Step [3600/4091], Loss: 1291779.3750\n",
      "Epoch [9/100], Step [3700/4091], Loss: 704248.5625\n",
      "Epoch [9/100], Step [3800/4091], Loss: 731332.1250\n",
      "Epoch [9/100], Step [3900/4091], Loss: 1247780.3750\n",
      "Epoch [9/100], Step [4000/4091], Loss: 787993.8125\n",
      "Epoch [10/100], Step [100/4091], Loss: 374881.6875\n",
      "Epoch [10/100], Step [200/4091], Loss: 1578970.3750\n",
      "Epoch [10/100], Step [300/4091], Loss: 705633.3125\n",
      "Epoch [10/100], Step [400/4091], Loss: 973013.9375\n",
      "Epoch [10/100], Step [500/4091], Loss: 530681.5000\n",
      "Epoch [10/100], Step [600/4091], Loss: 493858.5625\n",
      "Epoch [10/100], Step [700/4091], Loss: 701772.0625\n",
      "Epoch [10/100], Step [800/4091], Loss: 776420.1250\n",
      "Epoch [10/100], Step [900/4091], Loss: 435339.0938\n",
      "Epoch [10/100], Step [1000/4091], Loss: 1091040.2500\n",
      "Epoch [10/100], Step [1100/4091], Loss: 651801.7500\n",
      "Epoch [10/100], Step [1200/4091], Loss: 488810.6875\n",
      "Epoch [10/100], Step [1300/4091], Loss: 973571.2500\n",
      "Epoch [10/100], Step [1400/4091], Loss: 919533.8750\n",
      "Epoch [10/100], Step [1500/4091], Loss: 506407.3125\n",
      "Epoch [10/100], Step [1600/4091], Loss: 839033.5625\n",
      "Epoch [10/100], Step [1700/4091], Loss: 1181471.1250\n",
      "Epoch [10/100], Step [1800/4091], Loss: 1351205.3750\n",
      "Epoch [10/100], Step [1900/4091], Loss: 1073109.2500\n",
      "Epoch [10/100], Step [2000/4091], Loss: 663277.5625\n",
      "Epoch [10/100], Step [2100/4091], Loss: 345644.3438\n",
      "Epoch [10/100], Step [2200/4091], Loss: 829958.1250\n",
      "Epoch [10/100], Step [2300/4091], Loss: 1176735.5000\n",
      "Epoch [10/100], Step [2400/4091], Loss: 628545.3750\n",
      "Epoch [10/100], Step [2500/4091], Loss: 593503.6250\n",
      "Epoch [10/100], Step [2600/4091], Loss: 633801.3750\n",
      "Epoch [10/100], Step [2700/4091], Loss: 1032467.1250\n",
      "Epoch [10/100], Step [2800/4091], Loss: 543363.5000\n",
      "Epoch [10/100], Step [2900/4091], Loss: 624419.7500\n",
      "Epoch [10/100], Step [3000/4091], Loss: 433397.7500\n",
      "Epoch [10/100], Step [3100/4091], Loss: 699015.8750\n",
      "Epoch [10/100], Step [3200/4091], Loss: 860165.1250\n",
      "Epoch [10/100], Step [3300/4091], Loss: 905359.0625\n",
      "Epoch [10/100], Step [3400/4091], Loss: 918433.2500\n",
      "Epoch [10/100], Step [3500/4091], Loss: 1229962.7500\n",
      "Epoch [10/100], Step [3600/4091], Loss: 765731.1875\n",
      "Epoch [10/100], Step [3700/4091], Loss: 480922.3438\n",
      "Epoch [10/100], Step [3800/4091], Loss: 1762431.7500\n",
      "Epoch [10/100], Step [3900/4091], Loss: 965343.5000\n",
      "Epoch [10/100], Step [4000/4091], Loss: 369784.0625\n",
      "Epoch [11/100], Step [100/4091], Loss: 329421.9688\n",
      "Epoch [11/100], Step [200/4091], Loss: 939354.3750\n",
      "Epoch [11/100], Step [300/4091], Loss: 875941.8125\n",
      "Epoch [11/100], Step [400/4091], Loss: 463183.1875\n",
      "Epoch [11/100], Step [500/4091], Loss: 597143.0625\n",
      "Epoch [11/100], Step [600/4091], Loss: 1354628.7500\n",
      "Epoch [11/100], Step [700/4091], Loss: 327567.8438\n",
      "Epoch [11/100], Step [800/4091], Loss: 583562.8125\n",
      "Epoch [11/100], Step [900/4091], Loss: 561452.5625\n",
      "Epoch [11/100], Step [1000/4091], Loss: 1554678.7500\n",
      "Epoch [11/100], Step [1100/4091], Loss: 384546.1562\n",
      "Epoch [11/100], Step [1200/4091], Loss: 700050.9375\n",
      "Epoch [11/100], Step [1300/4091], Loss: 1151480.0000\n",
      "Epoch [11/100], Step [1400/4091], Loss: 636796.8125\n",
      "Epoch [11/100], Step [1500/4091], Loss: 582348.0000\n",
      "Epoch [11/100], Step [1600/4091], Loss: 335391.8750\n",
      "Epoch [11/100], Step [1700/4091], Loss: 1158915.8750\n",
      "Epoch [11/100], Step [1800/4091], Loss: 784026.0625\n",
      "Epoch [11/100], Step [1900/4091], Loss: 673129.0000\n",
      "Epoch [11/100], Step [2000/4091], Loss: 705774.2500\n",
      "Epoch [11/100], Step [2100/4091], Loss: 719417.0000\n",
      "Epoch [11/100], Step [2200/4091], Loss: 755941.1250\n",
      "Epoch [11/100], Step [2300/4091], Loss: 629334.7500\n",
      "Epoch [11/100], Step [2400/4091], Loss: 1020366.4375\n",
      "Epoch [11/100], Step [2500/4091], Loss: 962623.5625\n",
      "Epoch [11/100], Step [2600/4091], Loss: 1075814.1250\n",
      "Epoch [11/100], Step [2700/4091], Loss: 914554.5000\n",
      "Epoch [11/100], Step [2800/4091], Loss: 789396.8750\n",
      "Epoch [11/100], Step [2900/4091], Loss: 671001.7500\n",
      "Epoch [11/100], Step [3000/4091], Loss: 937515.2500\n",
      "Epoch [11/100], Step [3100/4091], Loss: 453621.5625\n",
      "Epoch [11/100], Step [3200/4091], Loss: 462521.6875\n",
      "Epoch [11/100], Step [3300/4091], Loss: 700392.9375\n",
      "Epoch [11/100], Step [3400/4091], Loss: 501089.7500\n",
      "Epoch [11/100], Step [3500/4091], Loss: 468631.1875\n",
      "Epoch [11/100], Step [3600/4091], Loss: 580806.1250\n",
      "Epoch [11/100], Step [3700/4091], Loss: 769875.6250\n",
      "Epoch [11/100], Step [3800/4091], Loss: 807232.6875\n",
      "Epoch [11/100], Step [3900/4091], Loss: 244578.2188\n",
      "Epoch [11/100], Step [4000/4091], Loss: 542547.4375\n",
      "Epoch [12/100], Step [100/4091], Loss: 426324.9375\n",
      "Epoch [12/100], Step [200/4091], Loss: 685004.0625\n",
      "Epoch [12/100], Step [300/4091], Loss: 786084.0625\n",
      "Epoch [12/100], Step [400/4091], Loss: 440360.4062\n",
      "Epoch [12/100], Step [500/4091], Loss: 702976.9375\n",
      "Epoch [12/100], Step [600/4091], Loss: 782504.5625\n",
      "Epoch [12/100], Step [700/4091], Loss: 1492697.1250\n",
      "Epoch [12/100], Step [800/4091], Loss: 537913.9375\n",
      "Epoch [12/100], Step [900/4091], Loss: 615278.7500\n",
      "Epoch [12/100], Step [1000/4091], Loss: 726722.7500\n",
      "Epoch [12/100], Step [1100/4091], Loss: 881320.6250\n",
      "Epoch [12/100], Step [1200/4091], Loss: 1233102.2500\n",
      "Epoch [12/100], Step [1300/4091], Loss: 834462.4375\n",
      "Epoch [12/100], Step [1400/4091], Loss: 802246.5625\n",
      "Epoch [12/100], Step [1500/4091], Loss: 1530413.7500\n",
      "Epoch [12/100], Step [1600/4091], Loss: 766540.9375\n",
      "Epoch [12/100], Step [1700/4091], Loss: 604532.3750\n",
      "Epoch [12/100], Step [1800/4091], Loss: 1009649.0000\n",
      "Epoch [12/100], Step [1900/4091], Loss: 648697.0625\n",
      "Epoch [12/100], Step [2000/4091], Loss: 1110114.1250\n",
      "Epoch [12/100], Step [2100/4091], Loss: 739857.9375\n",
      "Epoch [12/100], Step [2200/4091], Loss: 626299.8750\n",
      "Epoch [12/100], Step [2300/4091], Loss: 936596.5000\n",
      "Epoch [12/100], Step [2400/4091], Loss: 1410259.7500\n",
      "Epoch [12/100], Step [2500/4091], Loss: 523440.0312\n",
      "Epoch [12/100], Step [2600/4091], Loss: 1620307.0000\n",
      "Epoch [12/100], Step [2700/4091], Loss: 881713.6250\n",
      "Epoch [12/100], Step [2800/4091], Loss: 695442.8125\n",
      "Epoch [12/100], Step [2900/4091], Loss: 1140971.6250\n",
      "Epoch [12/100], Step [3000/4091], Loss: 528261.3750\n",
      "Epoch [12/100], Step [3100/4091], Loss: 693049.1875\n",
      "Epoch [12/100], Step [3200/4091], Loss: 794853.1875\n",
      "Epoch [12/100], Step [3300/4091], Loss: 441751.3750\n",
      "Epoch [12/100], Step [3400/4091], Loss: 718039.1250\n",
      "Epoch [12/100], Step [3500/4091], Loss: 1107112.1250\n",
      "Epoch [12/100], Step [3600/4091], Loss: 1030854.0000\n",
      "Epoch [12/100], Step [3700/4091], Loss: 1247310.2500\n",
      "Epoch [12/100], Step [3800/4091], Loss: 516773.7188\n",
      "Epoch [12/100], Step [3900/4091], Loss: 1561855.5000\n",
      "Epoch [12/100], Step [4000/4091], Loss: 514389.2188\n",
      "Epoch [13/100], Step [100/4091], Loss: 416936.1562\n",
      "Epoch [13/100], Step [200/4091], Loss: 696617.3125\n",
      "Epoch [13/100], Step [300/4091], Loss: 1575831.5000\n",
      "Epoch [13/100], Step [400/4091], Loss: 525635.1875\n",
      "Epoch [13/100], Step [500/4091], Loss: 503912.3125\n",
      "Epoch [13/100], Step [600/4091], Loss: 605571.6250\n",
      "Epoch [13/100], Step [700/4091], Loss: 1004023.0000\n",
      "Epoch [13/100], Step [800/4091], Loss: 714632.5625\n",
      "Epoch [13/100], Step [900/4091], Loss: 383129.0000\n",
      "Epoch [13/100], Step [1000/4091], Loss: 1206025.0000\n",
      "Epoch [13/100], Step [1100/4091], Loss: 395588.1562\n",
      "Epoch [13/100], Step [1200/4091], Loss: 825808.8750\n",
      "Epoch [13/100], Step [1300/4091], Loss: 577713.0000\n",
      "Epoch [13/100], Step [1400/4091], Loss: 526708.0000\n",
      "Epoch [13/100], Step [1500/4091], Loss: 1031253.1250\n",
      "Epoch [13/100], Step [1600/4091], Loss: 843693.9375\n",
      "Epoch [13/100], Step [1700/4091], Loss: 1119681.8750\n",
      "Epoch [13/100], Step [1800/4091], Loss: 800747.3125\n",
      "Epoch [13/100], Step [1900/4091], Loss: 1585318.7500\n",
      "Epoch [13/100], Step [2000/4091], Loss: 1069430.0000\n",
      "Epoch [13/100], Step [2100/4091], Loss: 682139.0000\n",
      "Epoch [13/100], Step [2200/4091], Loss: 567646.8125\n",
      "Epoch [13/100], Step [2300/4091], Loss: 1360696.0000\n",
      "Epoch [13/100], Step [2400/4091], Loss: 651886.1875\n",
      "Epoch [13/100], Step [2500/4091], Loss: 1140093.2500\n",
      "Epoch [13/100], Step [2600/4091], Loss: 1084873.6250\n",
      "Epoch [13/100], Step [2700/4091], Loss: 662813.0625\n",
      "Epoch [13/100], Step [2800/4091], Loss: 971929.5625\n",
      "Epoch [13/100], Step [2900/4091], Loss: 981093.6250\n",
      "Epoch [13/100], Step [3000/4091], Loss: 893352.8750\n",
      "Epoch [13/100], Step [3100/4091], Loss: 516105.2812\n",
      "Epoch [13/100], Step [3200/4091], Loss: 936851.2500\n",
      "Epoch [13/100], Step [3300/4091], Loss: 916895.0000\n",
      "Epoch [13/100], Step [3400/4091], Loss: 593135.6250\n",
      "Epoch [13/100], Step [3500/4091], Loss: 911688.6250\n",
      "Epoch [13/100], Step [3600/4091], Loss: 922933.0625\n",
      "Epoch [13/100], Step [3700/4091], Loss: 1041894.8125\n",
      "Epoch [13/100], Step [3800/4091], Loss: 824203.5000\n",
      "Epoch [13/100], Step [3900/4091], Loss: 1050954.0000\n",
      "Epoch [13/100], Step [4000/4091], Loss: 376592.1250\n",
      "Epoch [14/100], Step [100/4091], Loss: 925430.0000\n",
      "Epoch [14/100], Step [200/4091], Loss: 948719.6250\n",
      "Epoch [14/100], Step [300/4091], Loss: 1123269.3750\n",
      "Epoch [14/100], Step [400/4091], Loss: 755071.1250\n",
      "Epoch [14/100], Step [500/4091], Loss: 474012.0000\n",
      "Epoch [14/100], Step [600/4091], Loss: 865218.3125\n",
      "Epoch [14/100], Step [700/4091], Loss: 1799822.3750\n",
      "Epoch [14/100], Step [800/4091], Loss: 615082.5625\n",
      "Epoch [14/100], Step [900/4091], Loss: 1001830.5000\n",
      "Epoch [14/100], Step [1000/4091], Loss: 1231009.3750\n",
      "Epoch [14/100], Step [1100/4091], Loss: 602520.0625\n",
      "Epoch [14/100], Step [1200/4091], Loss: 917853.0625\n",
      "Epoch [14/100], Step [1300/4091], Loss: 785961.3125\n",
      "Epoch [14/100], Step [1400/4091], Loss: 1188491.0000\n",
      "Epoch [14/100], Step [1500/4091], Loss: 806011.7500\n",
      "Epoch [14/100], Step [1600/4091], Loss: 666619.8750\n",
      "Epoch [14/100], Step [1700/4091], Loss: 462899.9375\n",
      "Epoch [14/100], Step [1800/4091], Loss: 1204890.5000\n",
      "Epoch [14/100], Step [1900/4091], Loss: 525525.6250\n",
      "Epoch [14/100], Step [2000/4091], Loss: 887142.0000\n",
      "Epoch [14/100], Step [2100/4091], Loss: 694453.1250\n",
      "Epoch [14/100], Step [2200/4091], Loss: 1599322.6250\n",
      "Epoch [14/100], Step [2300/4091], Loss: 739967.1250\n",
      "Epoch [14/100], Step [2400/4091], Loss: 1324553.7500\n",
      "Epoch [14/100], Step [2500/4091], Loss: 1020530.0000\n",
      "Epoch [14/100], Step [2600/4091], Loss: 641682.2500\n",
      "Epoch [14/100], Step [2700/4091], Loss: 881030.1875\n",
      "Epoch [14/100], Step [2800/4091], Loss: 944856.4375\n",
      "Epoch [14/100], Step [2900/4091], Loss: 639283.3750\n",
      "Epoch [14/100], Step [3000/4091], Loss: 1156676.0000\n",
      "Epoch [14/100], Step [3100/4091], Loss: 1608156.7500\n",
      "Epoch [14/100], Step [3200/4091], Loss: 928178.8750\n",
      "Epoch [14/100], Step [3300/4091], Loss: 644795.1875\n",
      "Epoch [14/100], Step [3400/4091], Loss: 1079237.5000\n",
      "Epoch [14/100], Step [3500/4091], Loss: 652737.4375\n",
      "Epoch [14/100], Step [3600/4091], Loss: 1723751.8750\n",
      "Epoch [14/100], Step [3700/4091], Loss: 742632.5625\n",
      "Epoch [14/100], Step [3800/4091], Loss: 482695.4688\n",
      "Epoch [14/100], Step [3900/4091], Loss: 590837.6250\n",
      "Epoch [14/100], Step [4000/4091], Loss: 504440.0938\n",
      "Epoch [15/100], Step [100/4091], Loss: 1156349.1250\n",
      "Epoch [15/100], Step [200/4091], Loss: 544444.5000\n",
      "Epoch [15/100], Step [300/4091], Loss: 537988.2500\n",
      "Epoch [15/100], Step [400/4091], Loss: 914777.2500\n",
      "Epoch [15/100], Step [500/4091], Loss: 1312905.0000\n",
      "Epoch [15/100], Step [600/4091], Loss: 630083.2500\n",
      "Epoch [15/100], Step [700/4091], Loss: 472307.9688\n",
      "Epoch [15/100], Step [800/4091], Loss: 451575.1562\n",
      "Epoch [15/100], Step [900/4091], Loss: 1282112.2500\n",
      "Epoch [15/100], Step [1000/4091], Loss: 585246.4375\n",
      "Epoch [15/100], Step [1100/4091], Loss: 847284.6250\n",
      "Epoch [15/100], Step [1200/4091], Loss: 432769.8438\n",
      "Epoch [15/100], Step [1300/4091], Loss: 1082915.8750\n",
      "Epoch [15/100], Step [1400/4091], Loss: 888879.8125\n",
      "Epoch [15/100], Step [1500/4091], Loss: 1039259.5625\n",
      "Epoch [15/100], Step [1600/4091], Loss: 907564.9375\n",
      "Epoch [15/100], Step [1700/4091], Loss: 705378.8125\n",
      "Epoch [15/100], Step [1800/4091], Loss: 1029531.3125\n",
      "Epoch [15/100], Step [1900/4091], Loss: 655885.9375\n",
      "Epoch [15/100], Step [2000/4091], Loss: 776426.1875\n",
      "Epoch [15/100], Step [2100/4091], Loss: 1080676.6250\n",
      "Epoch [15/100], Step [2200/4091], Loss: 501065.1875\n",
      "Epoch [15/100], Step [2300/4091], Loss: 557576.5000\n",
      "Epoch [15/100], Step [2400/4091], Loss: 912666.8125\n",
      "Epoch [15/100], Step [2500/4091], Loss: 1544335.1250\n",
      "Epoch [15/100], Step [2600/4091], Loss: 999722.0000\n",
      "Epoch [15/100], Step [2700/4091], Loss: 890325.6250\n",
      "Epoch [15/100], Step [2800/4091], Loss: 455562.6250\n",
      "Epoch [15/100], Step [2900/4091], Loss: 587986.7500\n",
      "Epoch [15/100], Step [3000/4091], Loss: 1047228.8750\n",
      "Epoch [15/100], Step [3100/4091], Loss: 554385.1875\n",
      "Epoch [15/100], Step [3200/4091], Loss: 820081.0000\n",
      "Epoch [15/100], Step [3300/4091], Loss: 570844.0000\n",
      "Epoch [15/100], Step [3400/4091], Loss: 1164969.8750\n",
      "Epoch [15/100], Step [3500/4091], Loss: 616004.8750\n",
      "Epoch [15/100], Step [3600/4091], Loss: 397559.9062\n",
      "Epoch [15/100], Step [3700/4091], Loss: 461418.7188\n",
      "Epoch [15/100], Step [3800/4091], Loss: 1271912.7500\n",
      "Epoch [15/100], Step [3900/4091], Loss: 950351.0000\n",
      "Epoch [15/100], Step [4000/4091], Loss: 454064.5625\n",
      "Epoch [16/100], Step [100/4091], Loss: 720151.7500\n",
      "Epoch [16/100], Step [200/4091], Loss: 869301.9375\n",
      "Epoch [16/100], Step [300/4091], Loss: 650430.3750\n",
      "Epoch [16/100], Step [400/4091], Loss: 1207722.6250\n",
      "Epoch [16/100], Step [500/4091], Loss: 975212.8125\n",
      "Epoch [16/100], Step [600/4091], Loss: 727232.0000\n",
      "Epoch [16/100], Step [700/4091], Loss: 1380338.2500\n",
      "Epoch [16/100], Step [800/4091], Loss: 541816.3750\n",
      "Epoch [16/100], Step [900/4091], Loss: 1380825.7500\n",
      "Epoch [16/100], Step [1000/4091], Loss: 940957.8750\n",
      "Epoch [16/100], Step [1100/4091], Loss: 543099.2500\n",
      "Epoch [16/100], Step [1200/4091], Loss: 1158176.2500\n",
      "Epoch [16/100], Step [1300/4091], Loss: 1020667.6250\n",
      "Epoch [16/100], Step [1400/4091], Loss: 1235307.0000\n",
      "Epoch [16/100], Step [1500/4091], Loss: 572618.6875\n",
      "Epoch [16/100], Step [1600/4091], Loss: 600674.6250\n",
      "Epoch [16/100], Step [1700/4091], Loss: 596351.8750\n",
      "Epoch [16/100], Step [1800/4091], Loss: 1654560.2500\n",
      "Epoch [16/100], Step [1900/4091], Loss: 506676.6875\n",
      "Epoch [16/100], Step [2000/4091], Loss: 840450.5000\n",
      "Epoch [16/100], Step [2100/4091], Loss: 892649.9375\n",
      "Epoch [16/100], Step [2200/4091], Loss: 995084.6250\n",
      "Epoch [16/100], Step [2300/4091], Loss: 488095.5625\n",
      "Epoch [16/100], Step [2400/4091], Loss: 623217.0000\n",
      "Epoch [16/100], Step [2500/4091], Loss: 1133935.7500\n",
      "Epoch [16/100], Step [2600/4091], Loss: 959284.1875\n",
      "Epoch [16/100], Step [2700/4091], Loss: 600711.0000\n",
      "Epoch [16/100], Step [2800/4091], Loss: 1066648.5000\n",
      "Epoch [16/100], Step [2900/4091], Loss: 457958.7500\n",
      "Epoch [16/100], Step [3000/4091], Loss: 815010.8125\n",
      "Epoch [16/100], Step [3100/4091], Loss: 517584.3438\n",
      "Epoch [16/100], Step [3200/4091], Loss: 941371.3125\n",
      "Epoch [16/100], Step [3300/4091], Loss: 518768.1562\n",
      "Epoch [16/100], Step [3400/4091], Loss: 999437.7500\n",
      "Epoch [16/100], Step [3500/4091], Loss: 1292615.0000\n",
      "Epoch [16/100], Step [3600/4091], Loss: 257570.6094\n",
      "Epoch [16/100], Step [3700/4091], Loss: 1348293.3750\n",
      "Epoch [16/100], Step [3800/4091], Loss: 599484.8125\n",
      "Epoch [16/100], Step [3900/4091], Loss: 938685.1250\n",
      "Epoch [16/100], Step [4000/4091], Loss: 338417.6875\n",
      "Epoch [17/100], Step [100/4091], Loss: 717303.0000\n",
      "Epoch [17/100], Step [200/4091], Loss: 759356.7500\n",
      "Epoch [17/100], Step [300/4091], Loss: 734935.6250\n",
      "Epoch [17/100], Step [400/4091], Loss: 706552.4375\n",
      "Epoch [17/100], Step [500/4091], Loss: 1006533.5625\n",
      "Epoch [17/100], Step [600/4091], Loss: 927492.3750\n",
      "Epoch [17/100], Step [700/4091], Loss: 856061.1875\n",
      "Epoch [17/100], Step [800/4091], Loss: 1435386.6250\n",
      "Epoch [17/100], Step [900/4091], Loss: 799872.6875\n",
      "Epoch [17/100], Step [1000/4091], Loss: 770893.3125\n",
      "Epoch [17/100], Step [1100/4091], Loss: 963080.7500\n",
      "Epoch [17/100], Step [1200/4091], Loss: 1478132.5000\n",
      "Epoch [17/100], Step [1300/4091], Loss: 733628.0000\n",
      "Epoch [17/100], Step [1400/4091], Loss: 823576.5000\n",
      "Epoch [17/100], Step [1500/4091], Loss: 1383295.1250\n",
      "Epoch [17/100], Step [1600/4091], Loss: 787938.8750\n",
      "Epoch [17/100], Step [1700/4091], Loss: 947737.6250\n",
      "Epoch [17/100], Step [1800/4091], Loss: 562628.3750\n",
      "Epoch [17/100], Step [1900/4091], Loss: 1020109.2500\n",
      "Epoch [17/100], Step [2000/4091], Loss: 732538.6250\n",
      "Epoch [17/100], Step [2100/4091], Loss: 636969.7500\n",
      "Epoch [17/100], Step [2200/4091], Loss: 581570.8750\n",
      "Epoch [17/100], Step [2300/4091], Loss: 676895.1875\n",
      "Epoch [17/100], Step [2400/4091], Loss: 518800.2500\n",
      "Epoch [17/100], Step [2500/4091], Loss: 645460.1875\n",
      "Epoch [17/100], Step [2600/4091], Loss: 1020416.7500\n",
      "Epoch [17/100], Step [2700/4091], Loss: 661057.0000\n",
      "Epoch [17/100], Step [2800/4091], Loss: 1095412.2500\n",
      "Epoch [17/100], Step [2900/4091], Loss: 423597.9375\n",
      "Epoch [17/100], Step [3000/4091], Loss: 474228.4375\n",
      "Epoch [17/100], Step [3100/4091], Loss: 665282.8125\n",
      "Epoch [17/100], Step [3200/4091], Loss: 877495.5000\n",
      "Epoch [17/100], Step [3300/4091], Loss: 678196.0000\n",
      "Epoch [17/100], Step [3400/4091], Loss: 907043.6875\n",
      "Epoch [17/100], Step [3500/4091], Loss: 870473.0000\n",
      "Epoch [17/100], Step [3600/4091], Loss: 514471.3750\n",
      "Epoch [17/100], Step [3700/4091], Loss: 670042.3750\n",
      "Epoch [17/100], Step [3800/4091], Loss: 463701.3750\n",
      "Epoch [17/100], Step [3900/4091], Loss: 500174.1875\n",
      "Epoch [17/100], Step [4000/4091], Loss: 591597.3125\n",
      "Epoch [18/100], Step [100/4091], Loss: 766662.0625\n",
      "Epoch [18/100], Step [200/4091], Loss: 697206.0625\n",
      "Epoch [18/100], Step [300/4091], Loss: 417063.3750\n",
      "Epoch [18/100], Step [400/4091], Loss: 975674.3125\n",
      "Epoch [18/100], Step [500/4091], Loss: 801749.8750\n",
      "Epoch [18/100], Step [600/4091], Loss: 801041.9375\n",
      "Epoch [18/100], Step [700/4091], Loss: 693535.3750\n",
      "Epoch [18/100], Step [800/4091], Loss: 983024.2500\n",
      "Epoch [18/100], Step [900/4091], Loss: 477223.5312\n",
      "Epoch [18/100], Step [1000/4091], Loss: 599407.6875\n",
      "Epoch [18/100], Step [1100/4091], Loss: 979638.8750\n",
      "Epoch [18/100], Step [1200/4091], Loss: 631508.3125\n",
      "Epoch [18/100], Step [1300/4091], Loss: 1281020.0000\n",
      "Epoch [18/100], Step [1400/4091], Loss: 1061854.3750\n",
      "Epoch [18/100], Step [1500/4091], Loss: 1224105.1250\n",
      "Epoch [18/100], Step [1600/4091], Loss: 1219779.7500\n",
      "Epoch [18/100], Step [1700/4091], Loss: 864359.5000\n",
      "Epoch [18/100], Step [1800/4091], Loss: 381876.9688\n",
      "Epoch [18/100], Step [1900/4091], Loss: 1223434.8750\n",
      "Epoch [18/100], Step [2000/4091], Loss: 775192.0000\n",
      "Epoch [18/100], Step [2100/4091], Loss: 1427131.2500\n",
      "Epoch [18/100], Step [2200/4091], Loss: 743892.1875\n",
      "Epoch [18/100], Step [2300/4091], Loss: 1316255.6250\n",
      "Epoch [18/100], Step [2400/4091], Loss: 1061432.6250\n",
      "Epoch [18/100], Step [2500/4091], Loss: 565395.6250\n",
      "Epoch [18/100], Step [2600/4091], Loss: 837827.3750\n",
      "Epoch [18/100], Step [2700/4091], Loss: 947367.3750\n",
      "Epoch [18/100], Step [2800/4091], Loss: 803039.7500\n",
      "Epoch [18/100], Step [2900/4091], Loss: 995074.4375\n",
      "Epoch [18/100], Step [3000/4091], Loss: 1044282.0000\n",
      "Epoch [18/100], Step [3100/4091], Loss: 1326256.8750\n",
      "Epoch [18/100], Step [3200/4091], Loss: 921717.9375\n",
      "Epoch [18/100], Step [3300/4091], Loss: 1973096.8750\n",
      "Epoch [18/100], Step [3400/4091], Loss: 1088425.3750\n",
      "Epoch [18/100], Step [3500/4091], Loss: 593677.6250\n",
      "Epoch [18/100], Step [3600/4091], Loss: 546589.8750\n",
      "Epoch [18/100], Step [3700/4091], Loss: 953241.5000\n",
      "Epoch [18/100], Step [3800/4091], Loss: 1452344.6250\n",
      "Epoch [18/100], Step [3900/4091], Loss: 520349.8438\n",
      "Epoch [18/100], Step [4000/4091], Loss: 650854.8125\n",
      "Epoch [19/100], Step [100/4091], Loss: 1981091.1250\n",
      "Epoch [19/100], Step [200/4091], Loss: 577682.6250\n",
      "Epoch [19/100], Step [300/4091], Loss: 1472513.6250\n",
      "Epoch [19/100], Step [400/4091], Loss: 532637.6250\n",
      "Epoch [19/100], Step [500/4091], Loss: 616130.3750\n",
      "Epoch [19/100], Step [600/4091], Loss: 312002.0625\n",
      "Epoch [19/100], Step [700/4091], Loss: 1423485.0000\n",
      "Epoch [19/100], Step [800/4091], Loss: 860886.3125\n",
      "Epoch [19/100], Step [900/4091], Loss: 831241.0000\n",
      "Epoch [19/100], Step [1000/4091], Loss: 621627.2500\n",
      "Epoch [19/100], Step [1100/4091], Loss: 756909.0000\n",
      "Epoch [19/100], Step [1200/4091], Loss: 608643.3750\n",
      "Epoch [19/100], Step [1300/4091], Loss: 1061610.6250\n",
      "Epoch [19/100], Step [1400/4091], Loss: 596976.1250\n",
      "Epoch [19/100], Step [1500/4091], Loss: 709587.3750\n",
      "Epoch [19/100], Step [1600/4091], Loss: 986722.4375\n",
      "Epoch [19/100], Step [1700/4091], Loss: 746920.6250\n",
      "Epoch [19/100], Step [1800/4091], Loss: 367471.4375\n",
      "Epoch [19/100], Step [1900/4091], Loss: 547959.6875\n",
      "Epoch [19/100], Step [2000/4091], Loss: 497625.2500\n",
      "Epoch [19/100], Step [2100/4091], Loss: 548633.1875\n",
      "Epoch [19/100], Step [2200/4091], Loss: 892968.8750\n",
      "Epoch [19/100], Step [2300/4091], Loss: 829332.1250\n",
      "Epoch [19/100], Step [2400/4091], Loss: 645493.5000\n",
      "Epoch [19/100], Step [2500/4091], Loss: 1207113.3750\n",
      "Epoch [19/100], Step [2600/4091], Loss: 455670.0625\n",
      "Epoch [19/100], Step [2700/4091], Loss: 776842.0625\n",
      "Epoch [19/100], Step [2800/4091], Loss: 886559.0625\n",
      "Epoch [19/100], Step [2900/4091], Loss: 1139391.5000\n",
      "Epoch [19/100], Step [3000/4091], Loss: 413166.6562\n",
      "Epoch [19/100], Step [3100/4091], Loss: 640250.5000\n",
      "Epoch [19/100], Step [3200/4091], Loss: 672263.4375\n",
      "Epoch [19/100], Step [3300/4091], Loss: 1541855.8750\n",
      "Epoch [19/100], Step [3400/4091], Loss: 857360.4375\n",
      "Epoch [19/100], Step [3500/4091], Loss: 1037391.3750\n",
      "Epoch [19/100], Step [3600/4091], Loss: 949817.5000\n",
      "Epoch [19/100], Step [3700/4091], Loss: 786921.8125\n",
      "Epoch [19/100], Step [3800/4091], Loss: 1228891.5000\n",
      "Epoch [19/100], Step [3900/4091], Loss: 679837.1250\n",
      "Epoch [19/100], Step [4000/4091], Loss: 690754.8125\n",
      "Epoch [20/100], Step [100/4091], Loss: 915338.4375\n",
      "Epoch [20/100], Step [200/4091], Loss: 1039195.1875\n",
      "Epoch [20/100], Step [300/4091], Loss: 741621.0625\n",
      "Epoch [20/100], Step [400/4091], Loss: 1276017.3750\n",
      "Epoch [20/100], Step [500/4091], Loss: 724453.0000\n",
      "Epoch [20/100], Step [600/4091], Loss: 779896.2500\n",
      "Epoch [20/100], Step [700/4091], Loss: 825589.9375\n",
      "Epoch [20/100], Step [800/4091], Loss: 658494.8750\n",
      "Epoch [20/100], Step [900/4091], Loss: 730708.1875\n",
      "Epoch [20/100], Step [1000/4091], Loss: 425422.0938\n",
      "Epoch [20/100], Step [1100/4091], Loss: 1250601.2500\n",
      "Epoch [20/100], Step [1200/4091], Loss: 933811.7500\n",
      "Epoch [20/100], Step [1300/4091], Loss: 1109147.7500\n",
      "Epoch [20/100], Step [1400/4091], Loss: 927725.5625\n",
      "Epoch [20/100], Step [1500/4091], Loss: 1036354.3125\n",
      "Epoch [20/100], Step [1600/4091], Loss: 1382652.0000\n",
      "Epoch [20/100], Step [1700/4091], Loss: 606811.7500\n",
      "Epoch [20/100], Step [1800/4091], Loss: 743557.0000\n",
      "Epoch [20/100], Step [1900/4091], Loss: 656546.0625\n",
      "Epoch [20/100], Step [2000/4091], Loss: 876301.6250\n",
      "Epoch [20/100], Step [2100/4091], Loss: 1087149.0000\n",
      "Epoch [20/100], Step [2200/4091], Loss: 773951.5000\n",
      "Epoch [20/100], Step [2300/4091], Loss: 807933.7500\n",
      "Epoch [20/100], Step [2400/4091], Loss: 1335626.6250\n",
      "Epoch [20/100], Step [2500/4091], Loss: 1122728.2500\n",
      "Epoch [20/100], Step [2600/4091], Loss: 479386.8438\n",
      "Epoch [20/100], Step [2700/4091], Loss: 1139953.8750\n",
      "Epoch [20/100], Step [2800/4091], Loss: 1502417.0000\n",
      "Epoch [20/100], Step [2900/4091], Loss: 760961.6875\n",
      "Epoch [20/100], Step [3000/4091], Loss: 986580.5000\n",
      "Epoch [20/100], Step [3100/4091], Loss: 566784.8125\n",
      "Epoch [20/100], Step [3200/4091], Loss: 1401075.7500\n",
      "Epoch [20/100], Step [3300/4091], Loss: 1181730.7500\n",
      "Epoch [20/100], Step [3400/4091], Loss: 643899.5625\n",
      "Epoch [20/100], Step [3500/4091], Loss: 827641.5000\n",
      "Epoch [20/100], Step [3600/4091], Loss: 399835.7188\n",
      "Epoch [20/100], Step [3700/4091], Loss: 1092778.0000\n",
      "Epoch [20/100], Step [3800/4091], Loss: 1204865.8750\n",
      "Epoch [20/100], Step [3900/4091], Loss: 571169.4375\n",
      "Epoch [20/100], Step [4000/4091], Loss: 893838.5625\n",
      "Epoch [21/100], Step [100/4091], Loss: 461663.5000\n",
      "Epoch [21/100], Step [200/4091], Loss: 541553.7500\n",
      "Epoch [21/100], Step [300/4091], Loss: 1022766.9375\n",
      "Epoch [21/100], Step [400/4091], Loss: 494143.9062\n",
      "Epoch [21/100], Step [500/4091], Loss: 941055.2500\n",
      "Epoch [21/100], Step [600/4091], Loss: 1025771.1250\n",
      "Epoch [21/100], Step [700/4091], Loss: 346269.5000\n",
      "Epoch [21/100], Step [800/4091], Loss: 522634.8750\n",
      "Epoch [21/100], Step [900/4091], Loss: 947234.0000\n",
      "Epoch [21/100], Step [1000/4091], Loss: 749021.8750\n",
      "Epoch [21/100], Step [1100/4091], Loss: 948893.0625\n",
      "Epoch [21/100], Step [1200/4091], Loss: 584200.0000\n",
      "Epoch [21/100], Step [1300/4091], Loss: 405967.7500\n",
      "Epoch [21/100], Step [1400/4091], Loss: 385579.3750\n",
      "Epoch [21/100], Step [1500/4091], Loss: 902162.4375\n",
      "Epoch [21/100], Step [1600/4091], Loss: 658990.9375\n",
      "Epoch [21/100], Step [1700/4091], Loss: 726691.0625\n",
      "Epoch [21/100], Step [1800/4091], Loss: 759840.9375\n",
      "Epoch [21/100], Step [1900/4091], Loss: 1096190.6250\n",
      "Epoch [21/100], Step [2000/4091], Loss: 527137.4375\n",
      "Epoch [21/100], Step [2100/4091], Loss: 543997.6875\n",
      "Epoch [21/100], Step [2200/4091], Loss: 590864.8125\n",
      "Epoch [21/100], Step [2300/4091], Loss: 1387573.2500\n",
      "Epoch [21/100], Step [2400/4091], Loss: 708850.3750\n",
      "Epoch [21/100], Step [2500/4091], Loss: 762175.8750\n",
      "Epoch [21/100], Step [2600/4091], Loss: 879223.7500\n",
      "Epoch [21/100], Step [2700/4091], Loss: 731212.6875\n",
      "Epoch [21/100], Step [2800/4091], Loss: 809072.8750\n",
      "Epoch [21/100], Step [2900/4091], Loss: 738547.1250\n",
      "Epoch [21/100], Step [3000/4091], Loss: 598345.6875\n",
      "Epoch [21/100], Step [3100/4091], Loss: 495268.6562\n",
      "Epoch [21/100], Step [3200/4091], Loss: 843581.0000\n",
      "Epoch [21/100], Step [3300/4091], Loss: 546915.3125\n",
      "Epoch [21/100], Step [3400/4091], Loss: 379029.8125\n",
      "Epoch [21/100], Step [3500/4091], Loss: 810421.1250\n",
      "Epoch [21/100], Step [3600/4091], Loss: 1146766.2500\n",
      "Epoch [21/100], Step [3700/4091], Loss: 906546.4375\n",
      "Epoch [21/100], Step [3800/4091], Loss: 1472468.8750\n",
      "Epoch [21/100], Step [3900/4091], Loss: 676210.8125\n",
      "Epoch [21/100], Step [4000/4091], Loss: 626123.2500\n",
      "Epoch [22/100], Step [100/4091], Loss: 532889.8750\n",
      "Epoch [22/100], Step [200/4091], Loss: 519360.7500\n",
      "Epoch [22/100], Step [300/4091], Loss: 855227.8125\n",
      "Epoch [22/100], Step [400/4091], Loss: 372791.0938\n",
      "Epoch [22/100], Step [500/4091], Loss: 1243847.0000\n",
      "Epoch [22/100], Step [600/4091], Loss: 593274.1250\n",
      "Epoch [22/100], Step [700/4091], Loss: 837963.0000\n",
      "Epoch [22/100], Step [800/4091], Loss: 823024.8125\n",
      "Epoch [22/100], Step [900/4091], Loss: 766245.8125\n",
      "Epoch [22/100], Step [1000/4091], Loss: 853031.3750\n",
      "Epoch [22/100], Step [1100/4091], Loss: 2055499.1250\n",
      "Epoch [22/100], Step [1200/4091], Loss: 1104578.2500\n",
      "Epoch [22/100], Step [1300/4091], Loss: 804912.6250\n",
      "Epoch [22/100], Step [1400/4091], Loss: 791206.5625\n",
      "Epoch [22/100], Step [1500/4091], Loss: 411915.6250\n",
      "Epoch [22/100], Step [1600/4091], Loss: 981459.6875\n",
      "Epoch [22/100], Step [1700/4091], Loss: 834482.6875\n",
      "Epoch [22/100], Step [1800/4091], Loss: 724607.1250\n",
      "Epoch [22/100], Step [1900/4091], Loss: 1377954.5000\n",
      "Epoch [22/100], Step [2000/4091], Loss: 467348.5312\n",
      "Epoch [22/100], Step [2100/4091], Loss: 1046742.3125\n",
      "Epoch [22/100], Step [2200/4091], Loss: 831858.8750\n",
      "Epoch [22/100], Step [2300/4091], Loss: 943024.7500\n",
      "Epoch [22/100], Step [2400/4091], Loss: 562541.4375\n",
      "Epoch [22/100], Step [2500/4091], Loss: 748692.0000\n",
      "Epoch [22/100], Step [2600/4091], Loss: 596096.1250\n",
      "Epoch [22/100], Step [2700/4091], Loss: 1427399.5000\n",
      "Epoch [22/100], Step [2800/4091], Loss: 580256.9375\n",
      "Epoch [22/100], Step [2900/4091], Loss: 384815.2500\n",
      "Epoch [22/100], Step [3000/4091], Loss: 507472.1250\n",
      "Epoch [22/100], Step [3100/4091], Loss: 1113214.3750\n",
      "Epoch [22/100], Step [3200/4091], Loss: 964946.1250\n",
      "Epoch [22/100], Step [3300/4091], Loss: 1339349.7500\n",
      "Epoch [22/100], Step [3400/4091], Loss: 798837.8750\n",
      "Epoch [22/100], Step [3500/4091], Loss: 471400.0000\n",
      "Epoch [22/100], Step [3600/4091], Loss: 907589.9375\n",
      "Epoch [22/100], Step [3700/4091], Loss: 740017.8750\n",
      "Epoch [22/100], Step [3800/4091], Loss: 512906.0625\n",
      "Epoch [22/100], Step [3900/4091], Loss: 1285924.1250\n",
      "Epoch [22/100], Step [4000/4091], Loss: 1008975.0625\n",
      "Epoch [23/100], Step [100/4091], Loss: 844708.6875\n",
      "Epoch [23/100], Step [200/4091], Loss: 686343.3125\n",
      "Epoch [23/100], Step [300/4091], Loss: 540939.6875\n",
      "Epoch [23/100], Step [400/4091], Loss: 839813.8125\n",
      "Epoch [23/100], Step [500/4091], Loss: 409975.9688\n",
      "Epoch [23/100], Step [600/4091], Loss: 989778.0000\n",
      "Epoch [23/100], Step [700/4091], Loss: 1295721.7500\n",
      "Epoch [23/100], Step [800/4091], Loss: 584254.5000\n",
      "Epoch [23/100], Step [900/4091], Loss: 1332650.2500\n",
      "Epoch [23/100], Step [1000/4091], Loss: 1281347.3750\n",
      "Epoch [23/100], Step [1100/4091], Loss: 764065.5000\n",
      "Epoch [23/100], Step [1200/4091], Loss: 1330105.7500\n",
      "Epoch [23/100], Step [1300/4091], Loss: 647230.3750\n",
      "Epoch [23/100], Step [1400/4091], Loss: 599694.3750\n",
      "Epoch [23/100], Step [1500/4091], Loss: 1519720.0000\n",
      "Epoch [23/100], Step [1600/4091], Loss: 866148.6875\n",
      "Epoch [23/100], Step [1700/4091], Loss: 689466.9375\n",
      "Epoch [23/100], Step [1800/4091], Loss: 899984.6875\n",
      "Epoch [23/100], Step [1900/4091], Loss: 804509.5000\n",
      "Epoch [23/100], Step [2000/4091], Loss: 399996.9688\n",
      "Epoch [23/100], Step [2100/4091], Loss: 626080.8125\n",
      "Epoch [23/100], Step [2200/4091], Loss: 553044.8750\n",
      "Epoch [23/100], Step [2300/4091], Loss: 558956.9375\n",
      "Epoch [23/100], Step [2400/4091], Loss: 750165.0000\n",
      "Epoch [23/100], Step [2500/4091], Loss: 1035828.7500\n",
      "Epoch [23/100], Step [2600/4091], Loss: 859602.0000\n",
      "Epoch [23/100], Step [2700/4091], Loss: 1526167.1250\n",
      "Epoch [23/100], Step [2800/4091], Loss: 622917.1250\n",
      "Epoch [23/100], Step [2900/4091], Loss: 980507.5000\n",
      "Epoch [23/100], Step [3000/4091], Loss: 501931.4062\n",
      "Epoch [23/100], Step [3100/4091], Loss: 689525.5000\n",
      "Epoch [23/100], Step [3200/4091], Loss: 554309.8125\n",
      "Epoch [23/100], Step [3300/4091], Loss: 569703.0000\n",
      "Epoch [23/100], Step [3400/4091], Loss: 1281430.5000\n",
      "Epoch [23/100], Step [3500/4091], Loss: 1638991.0000\n",
      "Epoch [23/100], Step [3600/4091], Loss: 376019.5000\n",
      "Epoch [23/100], Step [3700/4091], Loss: 590384.5625\n",
      "Epoch [23/100], Step [3800/4091], Loss: 1611209.2500\n",
      "Epoch [23/100], Step [3900/4091], Loss: 1217307.6250\n",
      "Epoch [23/100], Step [4000/4091], Loss: 1269725.2500\n",
      "Epoch [24/100], Step [100/4091], Loss: 440681.4375\n",
      "Epoch [24/100], Step [200/4091], Loss: 494800.2188\n",
      "Epoch [24/100], Step [300/4091], Loss: 927171.8125\n",
      "Epoch [24/100], Step [400/4091], Loss: 494521.8750\n",
      "Epoch [24/100], Step [500/4091], Loss: 1245438.6250\n",
      "Epoch [24/100], Step [600/4091], Loss: 596054.2500\n",
      "Epoch [24/100], Step [700/4091], Loss: 491166.0625\n",
      "Epoch [24/100], Step [800/4091], Loss: 417615.1250\n",
      "Epoch [24/100], Step [900/4091], Loss: 817118.3750\n",
      "Epoch [24/100], Step [1000/4091], Loss: 558101.3750\n",
      "Epoch [24/100], Step [1100/4091], Loss: 1076814.0000\n",
      "Epoch [24/100], Step [1200/4091], Loss: 1076273.2500\n",
      "Epoch [24/100], Step [1300/4091], Loss: 819778.3125\n",
      "Epoch [24/100], Step [1400/4091], Loss: 884473.6250\n",
      "Epoch [24/100], Step [1500/4091], Loss: 733199.5000\n",
      "Epoch [24/100], Step [1600/4091], Loss: 513319.0000\n",
      "Epoch [24/100], Step [1700/4091], Loss: 543601.0000\n",
      "Epoch [24/100], Step [1800/4091], Loss: 729926.0625\n",
      "Epoch [24/100], Step [1900/4091], Loss: 736593.2500\n",
      "Epoch [24/100], Step [2000/4091], Loss: 1470053.0000\n",
      "Epoch [24/100], Step [2100/4091], Loss: 1700317.8750\n",
      "Epoch [24/100], Step [2200/4091], Loss: 605226.5625\n",
      "Epoch [24/100], Step [2300/4091], Loss: 832393.6250\n",
      "Epoch [24/100], Step [2400/4091], Loss: 944122.5625\n",
      "Epoch [24/100], Step [2500/4091], Loss: 798842.4375\n",
      "Epoch [24/100], Step [2600/4091], Loss: 438284.3125\n",
      "Epoch [24/100], Step [2700/4091], Loss: 1500271.0000\n",
      "Epoch [24/100], Step [2800/4091], Loss: 794511.3750\n",
      "Epoch [24/100], Step [2900/4091], Loss: 863670.7500\n",
      "Epoch [24/100], Step [3000/4091], Loss: 994068.1875\n",
      "Epoch [24/100], Step [3100/4091], Loss: 1313335.8750\n",
      "Epoch [24/100], Step [3200/4091], Loss: 815571.3750\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (inputs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m      9\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# Convert inputs to float\u001b[39;00m\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:419\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[1;32mIn[8], line 11\u001b[0m, in \u001b[0;36mInsuranceDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX\u001b[38;5;241m.\u001b[39miloc[idx]\u001b[38;5;241m.\u001b[39mvalues), torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39miloc[idx])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criteria = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.float()  # Convert inputs to float\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criteria(outputs, labels.float())  # Convert labels to float\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
